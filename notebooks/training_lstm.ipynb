{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14380465,"sourceType":"datasetVersion","datasetId":9152911,"isSourceIdPinned":true},{"sourceId":14414732,"sourceType":"datasetVersion","datasetId":9160442}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install neuralhydrology","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport copy\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Dict\nfrom torch import Tensor\nimport pandas as pd\n\nfrom neuralhydrology.modelzoo.handoff_forecast_lstm import HandoffForecastLSTM\nfrom neuralhydrology.utils.config import Config\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/full-hydro\"\n\nattrs = pd.read_csv(os.path.join(DATA_DIR, \"step5_static.csv\"))\nmeteo = pd.read_csv(os.path.join(DATA_DIR, \"step5_era5.csv\"))\nhres = pd.read_csv(os.path.join(DATA_DIR, \"step5_ecmwf.csv\"))\nq = pd.read_csv(os.path.join(DATA_DIR, \"step5_discharge.csv\"))\nprecip = pd.read_csv(os.path.join(DATA_DIR, \"step5_precip.csv\"))\n\nkeys = [\"grdc_no\", \"date\"]\n\nfor _df in [meteo, hres, q, precip]:\n    _df[\"date\"] = pd.to_datetime(_df[\"date\"])\n    \nflood_thr = (\n    q.dropna(subset=[\"q_obs\"])\n     .groupby(\"grdc_no\")[\"q_obs\"]\n     .quantile(0.95)\n     .to_dict()\n)\n\nUSE_METEO = True\nUSE_HRES = True\nUSE_NOAA = True\nUSE_STATIC = True\nUSE_LOG = True\n\ndf = None\n\ndf = meteo.copy()\n\nif USE_HRES:\n    df = df.merge(hres, on=keys, how=\"inner\")\n\nif USE_NOAA:\n    df = df.merge(precip, on=keys, how=\"inner\")\n\n# discharge luôn phải có\ndf = df.merge(q, on=keys, how=\"inner\")\n\nif USE_STATIC:\n    df = df.merge(attrs, on=\"grdc_no\", how=\"left\")\n    \ndf = df.sort_values([\"grdc_no\", \"date\"]).reset_index(drop=True)\n\nLOG1P_COLS = []\n\nif USE_METEO:\n    LOG1P_COLS += [\"tp_x\"]\n\nif USE_HRES:\n    LOG1P_COLS += [\"tp_y\"]\n\nif USE_NOAA:\n    LOG1P_COLS += [\"prcp_NOAA\"]\n\nLOG1P_COLS += [\"q_obs\"]\n\nfor c in LOG1P_COLS:\n    if c in df.columns:\n        df[c] = np.log1p(np.clip(df[c].to_numpy(), 0, None))\n    else:\n        print(f\"[warn] column not found, skipping log1p: {c}\")\n        \nprint(\"Total records:\", len(df))\ndf.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"METEO_BASE = [\"t2m\", \"tp\", \"sp\", \"ssr\", \"str\"]\nMETEO_COLS = [f\"{b}_x\" for b in METEO_BASE] + [\"prcp_NOAA\"]\nHRES_COLS  = [f\"{b}_y\" for b in METEO_BASE]\nTARGET = \"q_obs\"\n\nSTATIC_COLS = [\n    \"area\", \"mean_elevation\", \"mean_slope\",\n    \"clay_fraction\", \"sand_fraction\", \"silt_fraction\",\n    \"forest_cover\", \"urban_cover\", \"cropland_cover\",\n    \"aridity_index\", \"mean_precip\"\n]\n\nSEQ_H = 365\nSEQ_F = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_norm_stats(df, basin_ids):\n    df_sub = df[df[\"grdc_no\"].isin(basin_ids)].copy()\n\n    basin_static = df_sub.groupby(\"grdc_no\")[STATIC_COLS].first()\n\n    stats = {\n        \"ATTR_MEAN\": basin_static.mean(),\n        \"ATTR_STD\":  basin_static.std().replace(0, 1.0),\n\n        \"HRES_MEAN\": df_sub[HRES_COLS].mean(),\n        \"HRES_STD\":  df_sub[HRES_COLS].std().replace(0, 1.0),\n\n        \"METEO_MEAN\": df_sub[METEO_COLS].mean(),\n        \"METEO_STD\":  df_sub[METEO_COLS].std().replace(0, 1.0),\n    }\n    return stats","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_data_for_nh(df_basin, t_idx, stats):\n    df_basin = df_basin.sort_values(\"date\").reset_index(drop=True)\n\n    start = t_idx - SEQ_H\n    end   = t_idx + SEQ_F\n    assert start >= 0 and end <= len(df_basin)\n\n    window = df_basin.iloc[start:end]\n\n    hind = window.iloc[:SEQ_H]\n    fore = window.iloc[SEQ_H:]\n\n    x_d_hindcast = {}\n    for c in METEO_COLS:\n        vals = ((hind[c] - stats[\"METEO_MEAN\"][c]) / stats[\"METEO_STD\"][c]).to_numpy(dtype=np.float32)\n        x_d_hindcast[c] = torch.tensor(vals).unsqueeze(-1).unsqueeze(0)   # (1, SEQ_H, 1)\n\n    x_d_forecast = {}\n    for c in HRES_COLS:\n        vals = ((fore[c] - stats[\"HRES_MEAN\"][c]) / stats[\"HRES_STD\"][c]).to_numpy(dtype=np.float32)\n        x_d_forecast[c] = torch.tensor(vals).unsqueeze(-1).unsqueeze(0)   # (1, SEQ_F, 1)\n\n    # static attributes (normalized)\n    x_s_vals = ((window.iloc[0][STATIC_COLS] - stats[\"ATTR_MEAN\"]) / stats[\"ATTR_STD\"]).to_numpy(dtype=np.float32)\n    x_s = torch.tensor(x_s_vals).unsqueeze(0)\n\n    y = torch.tensor(\n        window[[TARGET]].values, dtype=torch.float32\n    ).unsqueeze(0)                  # (1, SEQ_H+SEQ_F, 1)\n\n    data = {\n        \"x_d_hindcast\": x_d_hindcast,\n        \"x_d_forecast\": x_d_forecast,\n        \"x_s\": x_s,\n        \"y\": y\n    }\n\n    return data\n\nprint(\"Total basins:\", df[\"grdc_no\"].nunique())\nprint(\"SEQ_H:\", SEQ_H, \"SEQ_F:\", SEQ_F)\ntotal_windows = 0\n\nfor basin, df_basin in df.groupby(\"grdc_no\"):\n    n_days = len(df_basin)\n    n_windows = max(0, n_days - SEQ_H - SEQ_F + 1)\n    total_windows += n_windows\nprint(\"Total samples after windowing:\", total_windows)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cfg = Config({\n    \"model\": \"HandoffForecastLSTM\",\n\n    # ----- sequence -----\n    \"seq_length\": SEQ_H + SEQ_F,      # 366\n    \"forecast_seq_length\": SEQ_F,     # 1\n\n    # ----- inputs -----\n    \"hindcast_inputs\": [METEO_COLS],\n    \"forecast_inputs\": [HRES_COLS],\n    \"dynamic_inputs\": [HRES_COLS + METEO_COLS],\n\n    \"static_attributes\": STATIC_COLS,\n\n    # ----- embeddings -----\n    \"statics_embedding\": {\n        \"type\": \"fc\",\n        \"hiddens\": [32],\n        \"activation\": \"tanh\",\n        \"dropout\": 0.0\n    },\n\n    \"dynamics_embedding\": {\n        \"type\": \"fc\",\n        \"hiddens\": [64],\n        \"activation\": \"tanh\",\n        \"dropout\": 0.0\n    },\n\n    # ----- model params -----\n    \"hidden_size\": 128,\n    \"initial_forget_bias\": 1.0,\n    \"output_dropout\": 0.0,\n\n    # ----- head -----\n    \"head\": \"regression\",\n    \"target_variables\": [TARGET],\n\n    # ----- required -----\n    \"nan_handling_method\": None,\n    \"timestep_counter\": False,\n\n    \"state_handoff_network\": {\n        \"type\": \"fc\",\n        \"hiddens\": [128],\n        \"activation\": \"tanh\",\n        \"dropout\": 0.0\n    },\n\n})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = HandoffForecastLSTM(cfg)\nmodel.train()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_basins(df):\n    basins = {}\n\n    for basin, df_basin in df.groupby(\"grdc_no\"):\n        df_basin = df_basin.sort_values(\"date\").reset_index(drop=True)\n\n        basins[basin] = {\n            \"df\": df_basin,\n            \"n_days\": len(df_basin)\n        }\n\n    return basins\n\nbasins = preprocess_basins(df)\nbasins_all = basins","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HydroWindowDataset(Dataset):\n    def __init__(self, basins, seq_h, seq_f):\n        self.samples = []\n        self.basins = basins\n        self.seq_h = seq_h\n        self.seq_f = seq_f\n\n        for basin, d in basins.items():\n            n = d[\"n_days\"]\n            for t in range(seq_h, n - seq_f + 1):\n                self.samples.append((basin, t))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n\ndataset = HydroWindowDataset(basins, SEQ_H, SEQ_F)\nprint(\"Total samples:\", len(dataset))\n\ndef make_collate_fn(basins_dict, stats):\n    def collate_fn(batch):\n        x_d_h, x_d_f, x_s, y = {}, {}, [], []\n        basin_ids = []\n        \n        for c in METEO_COLS:\n            x_d_h[c] = []\n        for c in HRES_COLS:\n            x_d_f[c] = []\n\n        for basin, t_idx in batch:\n            basin_ids.append(int(basin))\n            \n            df_basin = basins_dict[basin][\"df\"]\n            data = make_data_for_nh(df_basin, t_idx, stats)\n\n            for c in METEO_COLS:\n                x_d_h[c].append(data[\"x_d_hindcast\"][c][0])\n            for c in HRES_COLS:\n                x_d_f[c].append(data[\"x_d_forecast\"][c][0])\n\n            x_s.append(data[\"x_s\"][0])\n            y.append(data[\"y\"][0])\n\n        for k in x_d_h:\n            x_d_h[k] = torch.stack(x_d_h[k], dim=0)\n        for k in x_d_f:\n            x_d_f[k] = torch.stack(x_d_f[k], dim=0)\n\n        x_s = torch.stack(x_s, dim=0)\n        y   = torch.stack(y, dim=0)\n\n        return {\"basin_id\": torch.tensor(basin_ids, dtype=torch.long), \"x_d_hindcast\": x_d_h, \"x_d_forecast\": x_d_f, \"x_s\": x_s, \"y\": y}\n\n    return collate_fn\n\n\ndef train_one_epoch(model, loader, optimizer, loss_fn, device, epoch):\n    model.train()\n    total_loss = 0.0\n\n    for batch_idx, data in enumerate(loader, start=1):\n\n        # move to device\n        for k in data[\"x_d_hindcast\"]:\n            data[\"x_d_hindcast\"][k] = data[\"x_d_hindcast\"][k].to(device)\n        for k in data[\"x_d_forecast\"]:\n            data[\"x_d_forecast\"][k] = data[\"x_d_forecast\"][k].to(device)\n\n        data[\"x_s\"] = data[\"x_s\"].to(device)\n        data[\"y\"]   = data[\"y\"].to(device)\n\n        optimizer.zero_grad()\n\n        # forward\n        out = model(data)\n\n        y_true = data[\"y\"]\n        y_true_f = y_true[:, -cfg.forecast_seq_length:, :]\n\n        y_hat = out[\"y_hat\"]\n        y_pred = y_hat[:, -cfg.forecast_seq_length:, :]\n\n        loss = loss_fn(y_pred, y_true_f)\n\n        # backward\n        loss.backward()\n        optimizer.step()\n\n        batch_loss = loss.item()\n        total_loss += batch_loss\n\n        # LOG BATCH LOSS\n        if batch_idx % 50 == 0 or batch_idx == 1:\n            print(\n                f\"[Epoch {epoch:03d}] \"\n                f\"Batch {batch_idx:05d}/{len(loader)} | \"\n                f\"Batch Loss: {batch_loss:.6f}\"\n            )\n\n    # EPOCH LOSS\n    avg_loss = total_loss / len(loader)\n\n    print(\n        f\"===== Epoch {epoch:03d} DONE | \"\n        f\"Avg Loss: {avg_loss:.6f} =====\"\n    )\n\n    return avg_loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helpers","metadata":{}},{"cell_type":"code","source":"def basin_window_counts(basins_dict, seq_h, seq_f):\n    counts = {}\n    for b, d in basins_dict.items():\n        n = int(d[\"n_days\"])\n        counts[b] = max(0, n - seq_h - seq_f + 1)\n    return counts\n\ndef greedy_test_split_by_windows(basin_ids, counts, test_frac=0.10):\n    basin_ids = sorted(basin_ids, key=lambda b: counts.get(b, 0), reverse=True)\n    total = sum(counts.get(b, 0) for b in basin_ids)\n    target = test_frac * total\n\n    test, s = [], 0\n    for b in basin_ids:\n        if s < target:\n            test.append(b)\n            s += counts.get(b, 0)\n    train = [b for b in basin_ids if b not in set(test)]\n    return train, test\n\n@torch.no_grad()\ndef predict_all(model, loader, device):\n    model.eval()\n    ys, yps, bids = [], [], []\n\n    for data in loader:\n        # keep basin ids on CPU\n        b = data[\"basin_id\"]\n        if torch.is_tensor(b):\n            bids.append(b.cpu().numpy().reshape(-1))\n        else:\n            bids.append(np.asarray(b).reshape(-1))\n\n        for k in data[\"x_d_hindcast\"]:\n            data[\"x_d_hindcast\"][k] = data[\"x_d_hindcast\"][k].to(device)\n        for k in data[\"x_d_forecast\"]:\n            data[\"x_d_forecast\"][k] = data[\"x_d_forecast\"][k].to(device)\n\n        data[\"x_s\"] = data[\"x_s\"].to(device)\n        data[\"y\"]   = data[\"y\"].to(device)\n\n        out = model(data)\n\n        y_true = data[\"y\"][:, -cfg.forecast_seq_length:, :].detach().cpu().numpy().reshape(-1)\n        y_pred = out[\"y_hat\"][:, -cfg.forecast_seq_length:, :].detach().cpu().numpy().reshape(-1)\n\n        ys.append(y_true)\n        yps.append(y_pred)\n\n    return np.concatenate(bids, axis=0), np.concatenate(ys, axis=0), np.concatenate(yps, axis=0)\n\n@torch.no_grad()\ndef eval_one_epoch(model, loader, loss_fn, device):\n    model.eval()\n    total_loss = 0.0\n\n    for data in loader:\n        # move to device\n        for k in data[\"x_d_hindcast\"]:\n            data[\"x_d_hindcast\"][k] = data[\"x_d_hindcast\"][k].to(device)\n        for k in data[\"x_d_forecast\"]:\n            data[\"x_d_forecast\"][k] = data[\"x_d_forecast\"][k].to(device)\n\n        data[\"x_s\"] = data[\"x_s\"].to(device)\n        data[\"y\"]   = data[\"y\"].to(device)\n\n        out = model(data)\n\n        y_true = data[\"y\"]\n        y_true_f = y_true[:, -cfg.forecast_seq_length:, :]\n\n        y_pred = out[\"y_hat\"][:, -cfg.forecast_seq_length:, :]\n\n        loss = loss_fn(y_pred, y_true_f)\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\ndef regression_metrics(y_true, y_pred):\n    y_true = np.asarray(y_true).reshape(-1)\n    y_pred = np.asarray(y_pred).reshape(-1)\n\n    e = y_pred - y_true\n    mse = float(np.mean(e**2))\n    rmse = float(np.sqrt(mse))\n    bias = float(np.mean(e))\n\n    ss_res = float(np.sum((y_true - y_pred) ** 2))\n    ss_tot = float(np.sum((y_true - np.mean(y_true)) ** 2))\n\n    r = np.corrcoef(y_true, y_pred)[0, 1]\n\n    nse = float(1.0 - ss_res / ss_tot) if ss_tot > 0 else float(\"nan\")\n\n    return {\"MSE\": mse, \"RMSE\": rmse, \"Bias\": bias, \"r\": r, \"NSE\": nse}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"counts = basin_window_counts(basins_all, SEQ_H, SEQ_F)\nbasin_ids = list(basins_all.keys())\n\ntrainval_ids, test_ids = greedy_test_split_by_windows(basin_ids, counts, test_frac=0.10)\n\nval_frac_of_trainval = 0.1 / 0.9\ntrain_ids, val_ids = greedy_test_split_by_windows(trainval_ids, counts, test_frac=val_frac_of_trainval)\n\ndef _count_windows(ids):\n    return sum(counts[b] for b in ids)\n\nn_all  = _count_windows(basin_ids)\nn_tr   = _count_windows(train_ids)\nn_val  = _count_windows(val_ids)\nn_test = _count_windows(test_ids)\n\nprint(\"Basins total:\", len(basin_ids))\nprint(\"Train basins:\", len(train_ids), \"windows:\", n_tr, \"frac:\", n_tr/n_all)\nprint(\"Val basins:\", len(val_ids), \"windows:\", n_val, \"frac:\", n_val/n_all)\nprint(\"Test basins:\", len(test_ids), \"windows:\", n_test,\"frac:\", n_test/n_all)\n\n\nBATCH_SIZE = 8\nMAX_EPOCHS = 50\nPATIENCE = 15\nMIN_DELTA = 1e-6\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ntrain_loss_fn = torch.nn.MSELoss()\nval_loss_fn = torch.nn.MSELoss()\n\ncfg_base = cfg\ncfg_dict_base = cfg.as_dict()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_hp = {'name': 'hs256_lr1e-3_wd1e-4_do0.1', 'hidden_size': 128, 'lr': 0.0001, 'weight_decay': 0.0001, 'output_dropout': 0.0}\nfinal_epochs = MAX_EPOCHS\nbasins_train = {b: basins_all[b] for b in train_ids}\nbasins_val = {b: basins_all[b] for b in val_ids}\nbasins_test = {b: basins_all[b] for b in test_ids}\n\nstats_train = compute_norm_stats(df, train_ids)\nbasins = basins_train\ntrain_ds = HydroWindowDataset(basins, SEQ_H, SEQ_F)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0,\n                             collate_fn=make_collate_fn(basins, stats_train))\nbasins = basins_val\nval_ds = HydroWindowDataset(basins, SEQ_H, SEQ_F)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n                            collate_fn=make_collate_fn(basins, stats_train))\n\ncfg_final_dict = copy.deepcopy(cfg_dict_base)\ncfg_final_dict[\"hidden_size\"] = best_hp[\"hidden_size\"]\ncfg_final_dict[\"output_dropout\"] = best_hp[\"output_dropout\"]\ncfg = Config(cfg_final_dict)\n\nfinal_model = HandoffForecastLSTM(cfg).to(device)\nfinal_opt = torch.optim.Adam(final_model.parameters(), lr=best_hp[\"lr\"], weight_decay=best_hp[\"weight_decay\"])\n\nbest_val = float(\"inf\")\nbest_state = None\nbad_epochs = 0\n\nhistory = {\"epoch\": [], \"train_loss\": [], \"val_loss\": []}\n\nfor epoch in range(1, MAX_EPOCHS + 1):\n    train_loss = train_one_epoch(final_model, train_loader, final_opt, train_loss_fn, device, epoch)\n    val_loss = eval_one_epoch(final_model, val_loader, val_loss_fn, device)\n\n    history[\"epoch\"].append(epoch)\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n\n    print(f\"[Epoch {epoch:03d}] train_loss={train_loss:.6f}  val_loss={val_loss:.6f}\")\n\n    if val_loss < best_val - MIN_DELTA:\n        best_val = val_loss\n        best_state = copy.deepcopy(final_model.state_dict())\n        bad_epochs = 0\n    else:\n        bad_epochs += 1\n        if bad_epochs >= PATIENCE:\n            print(f\"Early stopping at epoch {epoch:03d} (best val_loss={best_val:.6f})\")\n            break\n\n# load best checkpoint\nif best_state is not None:\n    final_model.load_state_dict(best_state)\n\nplt.figure()\nplt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train\")\nplt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"val\")\nplt.title(\"Loss Plot\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.savefig(\"/kaggle/working/loss.png\", dpi=200)\nplt.show()\n\n# Evaluate on test set\nbasins_test = {b: basins_all[b] for b in test_ids}\nbasins = basins_test\ntest_ds = HydroWindowDataset(basins, SEQ_H, SEQ_F)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0,\n                         collate_fn=make_collate_fn(basins_test, stats_train))\n\n\nbasin_id, y_true_log, y_pred_log = predict_all(final_model, test_loader, device)\n\n# metrics in log space\nm_log = regression_metrics(y_true_log, y_pred_log)\n\n# invert log1p back to original discharge units\ny_true_q = np.expm1(y_true_log)\ny_pred_q = np.expm1(y_pred_log)\ny_true_q = np.clip(y_true_q, 0, None)\ny_pred_q = np.clip(y_pred_q, 0, None)\n\nm_q = regression_metrics(y_true_q, y_pred_q)\nprint(\"\\nTEST metrics (log1p space):\", m_log)\nprint(\"TEST metrics (original q):\", m_q)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs(\"outputs\", exist_ok=True)\nsave_path = \"outputs/handoff_lstm_best_cv.pt\"\n\ntorch.save(\n    {\n        \"state_dict\": final_model.state_dict(),\n        \"cfg\": cfg.as_dict(),\n        \"best_hp\": best_hp,\n        \"final_epochs\": final_epochs,\n        \"test_metrics_log\": m_log,\n        \"test_metrics_q\": m_q,\n        \"test_basins\": test_ids\n    },\n    save_path\n)\n\nprint(\"Saved model to:\", save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}